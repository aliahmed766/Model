{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85b5a388-c117-42cb-806f-9aa35a54f5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Accuracy: 0.8653980582524272\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.87      0.87      6793\n",
      "           1       0.86      0.86      0.86      6082\n",
      "\n",
      "    accuracy                           0.87     12875\n",
      "   macro avg       0.86      0.86      0.86     12875\n",
      "weighted avg       0.87      0.87      0.87     12875\n",
      "\n",
      "\n",
      "🔍 Customer Churn Prediction\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Age:  22\n",
      "Enter Tenure:  2003\n",
      "Enter Monthly Usage:  23\n",
      "Enter Total Spend:  20000\n",
      "Enter Contract Length:  3232\n",
      "Enter Gender (Male/Female):  male\n",
      "Enter Last Interaction (days):  2\n",
      "Enter Subscription Type (Basic/Premium/Other):  Basic\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Monthly Usage\nFeature names seen at fit time, yet now missing:\n- CustomerID\n- Payment Delay\n- Support Calls\n- Usage Frequency\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     51\u001b[39m input_data = pd.DataFrame({\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mAge\u001b[39m\u001b[33m\"\u001b[39m: [age],\n\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mTenure\u001b[39m\u001b[33m\"\u001b[39m: [tenure],\n\u001b[32m   (...)\u001b[39m\u001b[32m     59\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mSubscription Type\u001b[39m\u001b[33m\"\u001b[39m: [\u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subscription_type.lower() == \u001b[33m\"\u001b[39m\u001b[33mpremium\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[32m0\u001b[39m]\n\u001b[32m     60\u001b[39m })\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# Step 11: Apply same scaling\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m input_scaled = \u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Step 12: Predict churn\u001b[39;00m\n\u001b[32m     66\u001b[39m prediction = model.predict(input_scaled)[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\NAVTTC\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:316\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    314\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    318\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    319\u001b[39m         return_tuple = (\n\u001b[32m    320\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    321\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    322\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\NAVTTC\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1075\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1072\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1074\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1075\u001b[39m X = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1076\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1077\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1078\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1079\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1080\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1081\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1082\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1083\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1084\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1087\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\NAVTTC\\Lib\\site-packages\\sklearn\\utils\\validation.py:2929\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2845\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_data\u001b[39m(\n\u001b[32m   2846\u001b[39m     _estimator,\n\u001b[32m   2847\u001b[39m     /,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2853\u001b[39m     **check_params,\n\u001b[32m   2854\u001b[39m ):\n\u001b[32m   2855\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Validate input data and set or check feature names and counts of the input.\u001b[39;00m\n\u001b[32m   2856\u001b[39m \n\u001b[32m   2857\u001b[39m \u001b[33;03m    This helper function should be used in an estimator that requires input\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   2927\u001b[39m \u001b[33;03m        validated.\u001b[39;00m\n\u001b[32m   2928\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2929\u001b[39m     \u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_estimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2930\u001b[39m     tags = get_tags(_estimator)\n\u001b[32m   2931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tags.target_tags.required:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\anaconda3\\envs\\NAVTTC\\Lib\\site-packages\\sklearn\\utils\\validation.py:2787\u001b[39m, in \u001b[36m_check_feature_names\u001b[39m\u001b[34m(estimator, X, reset)\u001b[39m\n\u001b[32m   2784\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[32m   2785\u001b[39m     message += \u001b[33m\"\u001b[39m\u001b[33mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[31mValueError\u001b[39m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- Monthly Usage\nFeature names seen at fit time, yet now missing:\n- CustomerID\n- Payment Delay\n- Support Calls\n- Usage Frequency\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 2: Load dataset\n",
    "data = pd.read_csv(\"customer_churn_dataset-testing-master.csv\")\n",
    "\n",
    "# Step 3: Encode categorical columns\n",
    "le = LabelEncoder()\n",
    "for col in data.select_dtypes(include='object').columns:\n",
    "    data[col] = le.fit_transform(data[col])\n",
    "\n",
    "# Step 4: Split features and target\n",
    "X = data.drop(\"Churn\", axis=1)\n",
    "y = data[\"Churn\"]\n",
    "\n",
    "# Step 5: Split into train/test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 6: Scale numeric features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Step 7: Train the model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Step 8: Evaluate model\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"✅ Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "# Step 9: User input (interactive in Jupyter)\n",
    "print(\"\\n🔍 Customer Churn Prediction\\n\")\n",
    "\n",
    "age = float(input(\"Enter Age: \"))\n",
    "tenure = float(input(\"Enter Tenure: \"))\n",
    "usage = float(input(\"Enter Monthly Usage: \"))\n",
    "spend = float(input(\"Enter Total Spend: \"))\n",
    "contract_length = float(input(\"Enter Contract Length: \"))\n",
    "gender = input(\"Enter Gender (Male/Female): \")\n",
    "last_interaction = float(input(\"Enter Last Interaction (days): \"))\n",
    "subscription_type = input(\"Enter Subscription Type (Basic/Premium/Other): \")\n",
    "\n",
    "# Step 10: Match training feature structure\n",
    "# Create a DataFrame with all required features\n",
    "input_data = pd.DataFrame({\n",
    "    \"Age\": [age],\n",
    "    \"Tenure\": [tenure],\n",
    "    \"Monthly Usage\": [usage],\n",
    "    \"Total Spend\": [spend],\n",
    "    \"Contract Length\": [contract_length],\n",
    "    \"Gender\": [1 if gender.lower() == \"male\" else 0],\n",
    "    \"Last Interaction\": [last_interaction],\n",
    "    \"Subscription Type\": [1 if subscription_type.lower() == \"premium\" else 0]\n",
    "})\n",
    "\n",
    "# Step 11: Apply same scaling\n",
    "input_scaled = scaler.transform(input_data)\n",
    "\n",
    "# Step 12: Predict churn\n",
    "prediction = model.predict(input_scaled)[0]\n",
    "probability = model.predict_proba(input_scaled)[0][1]\n",
    "\n",
    "# Step 13: Output result\n",
    "if prediction == 1:\n",
    "    print(f\"\\n⚠️ The customer is likely to CHURN. (Probability: {probability:.2f})\")\n",
    "else:\n",
    "    print(f\"\\n✅ The customer is NOT likely to churn. (Probability: {probability:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25f78cb-68b2-4543-bfef-1eb8d275319f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8d0a3f-6880-49ac-9e02-f20c364621f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc17e398-a156-4fb9-9118-524774c1fd93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819a63d5-3b95-4d1d-9edd-c07522633260",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NAVTTC)",
   "language": "python",
   "name": "navttc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
